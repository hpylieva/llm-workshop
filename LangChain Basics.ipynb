{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hpylieva/llm-workshop/blob/main/LangChain%20Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building A Language Model Application"
      ],
      "metadata": {
        "id": "3CONjBYGh0bY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing all the necessary packages..."
      ],
      "metadata": {
        "id": "IbYVgIZchwfq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7072f92-e45d-4ded-df3c-02cd38b383f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain huggingface_hub openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up OpenAI model"
      ],
      "metadata": {
        "id": "M6yiwXNnvzxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to load some tokens. I've put them in a file of the following structure:\n",
        "```\n",
        "{\n",
        "\"OPENAI_API_KEY\": \"...\",\n",
        "\"HUGGINGFACEHUB_API_TOKEN\": \"...\",\n",
        "\"SERPAPI_API_KEY\":\"...\"\n",
        "}\n",
        "\n",
        "```\n",
        "Then I just added this file to a context of this notebook."
      ],
      "metadata": {
        "id": "w2PbNnQhqzAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "with open('creds.json') as file:\n",
        "  creds = json.load(file)\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "J1rye0_OetAU"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For most LLMs we can set up  temperature parameter whcih controls the creativity of the text generated by the OpenAI API. A higher temperature will produce more creative text, while a lower temperature will produce more predictable text.\n",
        "\n",
        "Let's make our outputs quite predictable, but with a bit of creativity.\n",
        "\n",
        "The default value of temperature is mostly 0.7."
      ],
      "metadata": {
        "id": "OaabjHKmh7_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overal_temperature = 0.1"
      ],
      "metadata": {
        "id": "mu6rG38mhFYo"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find decsription of parameters of OpenAI models [here](https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html).\n",
        "\n",
        "And regarding OpenAI models with LangChain integration - [here](https://python.langchain.com/docs/integrations/llms/openai)"
      ],
      "metadata": {
        "id": "3wYGExb_ioxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=overal_temperature)"
      ],
      "metadata": {
        "id": "-lzO5PfUpwfv"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting predictions"
      ],
      "metadata": {
        "id": "zMaNWpGMjhc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm('who am I talking to?')"
      ],
      "metadata": {
        "id": "fI_Ve5bjjA2D",
        "outputId": "51a30243-bf71-4284-8ac4-fc033a509986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nYou are talking to a person who is using this website.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "request = \"What are 5 vacation destinations for someone who likes to eat pasta?\"\n",
        "print(llm(request))"
      ],
      "metadata": {
        "id": "sqF-YPVEi7jZ",
        "outputId": "febbf7ed-9d13-49f4-f9b3-4cf8fb685442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Rome, Italy\n",
            "2. Bologna, Italy\n",
            "3. Naples, Italy\n",
            "4. Florence, Italy\n",
            "5. Venice, Italy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates: Manage prompts for LLMs"
      ],
      "metadata": {
        "id": "ulXC2DyLjtw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "MP2yGFHej1Qr"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"food\"],\n",
        "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "pPq0lUpoj33z"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(food=\"donuts\"))"
      ],
      "metadata": {
        "id": "IPTvCstej7WP",
        "outputId": "74f93324-efde-42f6-a62e-197381334326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are 5 vacation destinations for someone who likes to eat donuts?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.predict(prompt.format(food=\"donuts\")))"
      ],
      "metadata": {
        "id": "qA1FWh13j-uH",
        "outputId": "74a22e28-a42b-4ee5-a821-d0592532f27d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Portland, Oregon - Home to the famous Voodoo Doughnuts.\n",
            "2. Berlin, Germany - Home to the world-famous Berliner donuts.\n",
            "3. Tokyo, Japan - Home to the unique and delicious taiyaki donuts.\n",
            "4. New York City, USA - Home to the iconic Doughnut Plant.\n",
            "5. Montreal, Canada - Home to the famous St-Viateur Bagel & Donut shop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chains: Combine LLMs and prompts in multi-step workflows"
      ],
      "metadata": {
        "id": "SbelXktTkzRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "BF43XLvlk3aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "print(chain.run(\"fresh fish\"))"
      ],
      "metadata": {
        "id": "0jyUSrYpknCV",
        "outputId": "0d3a5158-f348-41b2-dd18-e451dfc9d0c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Vancouver, Canada\n",
            "2. Cancun, Mexico\n",
            "3. Honolulu, Hawaii\n",
            "4. San Sebastian, Spain\n",
            "5. Sydney, Australia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agents: Dynamically call chains based on user input"
      ],
      "metadata": {
        "id": "lQxNVxu9lU0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "x3_j3IHYlXT7",
        "outputId": "286419db-1777-4436-97b6-3b75e213e008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make search API available (100 searches per month) just register [here](https://serpapi.com/users/welcome)"
      ],
      "metadata": {
        "id": "852Ucalcrgh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent"
      ],
      "metadata": {
        "id": "CeVfjVALlUjl"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERPAPI_API_KEY\"]"
      ],
      "metadata": {
        "id": "OepTDXlQmvvu"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "rhrz11BSlhkl"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, let's initialize an agent with:\n",
        "# 1. The tools\n",
        "# 2. The language model\n",
        "# 3. The type of agent we want to use.\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "uo_2mVrSm3zV"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See list of agents types [here](https://python.langchain.com/docs/modules/agents/agent_types/)"
      ],
      "metadata": {
        "id": "aQQP-n52m9EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's test it out!\n",
        "agent.run(\"Who is the current leader of China? What is the largest prime number that is smaller than their age?\")"
      ],
      "metadata": {
        "id": "lklHRp6om-Q0",
        "outputId": "b2a112c7-fc80-47e3-ca76-ba8d055e50ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use a search engine to find out the leader's age first. \n",
            "Action: Search\n",
            "Action Input: Who is the current leader of China?\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mXi Jinping (Chinese: ‰π†ËøëÂπ≥; pinyin: X√≠ J√¨np√≠ng; born 15 June 1953) is a Chinese politician who has been serving as the general secretary of the Chinese Communist Party (CCP) and chairman of the Central Military Commission (CMC), and thus as the paramount leader of China, since 2012.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use a calculator to find the largest prime number that is smaller than the leader's age.\n",
            "Action: Calculator\n",
            "Action Input: The age of Xi Jinping (66)\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 66\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The largest prime number smaller than the age of Xi Jinping is 66.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The largest prime number smaller than the age of Xi Jinping is 66.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory: Add state to chains and agents\n"
      ],
      "metadata": {
        "id": "vV7EY8hirPoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import ConversationChain"
      ],
      "metadata": {
        "id": "iB7zyBbwrMKF"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm, verbose=True)"
      ],
      "metadata": {
        "id": "0f60XDVArU5i"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi there!\")"
      ],
      "metadata": {
        "id": "jMUElaD6rXaH",
        "outputId": "f3f032bb-8fad-4baf-b21f-b7494d180d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"My name is Hanna. What date is it today?\")"
      ],
      "metadata": {
        "id": "sCBmf389r7ZS",
        "outputId": "29cb32c3-7694-4add-b76b-69be18213cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
            "Human: My name is Hanna. What date is it today?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Today is April 15th, 2021. It's a Tuesday.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What was the first thing I said to you?\")"
      ],
      "metadata": {
        "id": "MRmBXSC1sHtA",
        "outputId": "52caac96-f845-42e4-875d-c628e05ce097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
            "Human: My name is Hanna. What date is it today?\n",
            "AI:  Today is April 15th, 2021. It's a Tuesday.\n",
            "Human: What was the first thing I said to you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' You said \"Hi there!\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finally: Comparing and Evaluating LLMs"
      ],
      "metadata": {
        "id": "e--hMIfWIwsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = creds['HUGGINGFACEHUB_API_TOKEN']"
      ],
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the LLMs"
      ],
      "metadata": {
        "id": "qjtcjrq7PnAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up Flan models\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "\n",
        "flan_20B = HuggingFaceHub(repo_id=\"google/flan-ul2\",\n",
        "                         model_kwargs={\"temperature\":overal_temperature,\n",
        "                                       \"max_new_tokens\":200}\n",
        "                         )"
      ],
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5xxl = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\",\n",
        "                         model_kwargs={\"temperature\":overal_temperature,\n",
        "                                       \"max_new_tokens\":200}\n",
        "                         )"
      ],
      "metadata": {
        "id": "ys9FQLsISSCK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up a comparison lab"
      ],
      "metadata": {
        "id": "MDWW8nDERcGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.model_laboratory import ModelLaboratory"
      ],
      "metadata": {
        "id": "sy4s37W9m7X6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "zZUMGKuvn_HV"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_list = [\n",
        "    flan_20B,\n",
        "    flan_t5xxl,\n",
        "    llm\n",
        "]"
      ],
      "metadata": {
        "id": "N5jHtwgkabw3"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ],
      "metadata": {
        "id": "9s07AIuJm_Gv"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run it on some and compare!"
      ],
      "metadata": {
        "id": "LDQ8VcedrkIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare(\"What is the opposite of up?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eb6pNpimfu6",
        "outputId": "0a85b58f-4a02-4b78-ef17-ddabdbb278b2"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "What is the opposite of up?\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mDown is the opposite of up. Up is a direction. Down is a location. The answer: down.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mDown is the opposite of up. Down is the direction of a downward motion. The answer: down.\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAI\u001b[0m\n",
            "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n",
            "\u001b[38;5;200m\u001b[1;3m The opposite of up is down.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare(\"Answer the following question by reasoning step by step. The cafeteria had 23 apples. \\\n",
        "If they used 20 for lunch, and bought 6 more, how many apple do they have?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puWRd2nwT5eD",
        "outputId": "94ae3146-f474-44c7-bfc9-168e4aa366b7"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mThey had 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. So the answer is 9.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mThe cafeteria had 23 - 20 = 3 apples after lunch. They bought 6 + 3 = 7 apples. The cafeteria has 7 - 3 = 2 apples.\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAI\u001b[0m\n",
            "Params: {'model_name': 'text-davinci-003', 'temperature': 0.1, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n",
            "\u001b[38;5;200m\u001b[1;3m \n",
            "\n",
            "Step 1: The cafeteria had 23 apples. \n",
            "\n",
            "Step 2: They used 20 for lunch. \n",
            "\n",
            "Step 3: They bought 6 more. \n",
            "\n",
            "Step 4: So, they now have 23 + 6 = 29 apples.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''\n",
        "Can Elon Musk have a conversation with George Washington? Give the rationale before answering.\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP63DmDbaY_X",
        "outputId": "9242cf98-371a-4026-d16d-1ea3fbd2ae87"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "\n",
            "Can Elon Musk have a conversation with George Washington? Give the rationale before answering.\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mGeorge Washington died in 1799. Elon Musk was born in 1971. So, the final answer is no.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mElon Musk is a billionaire entrepreneur who has developed a time machine. The time machine allows him to travel back in time and communicate with people who have died. George Washington died in 1799. So the final answer is no.\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3mFirst, we need to establish that George Washington is no longer alive. He passed away in 1799. Therefore, it is not possible for Elon Musk to have a direct conversation with him in the traditional sense.\n",
            "\n",
            "However, if we consider the possibility of time travel or some form of advanced technology that allows communication with individuals from the past, it is still highly unlikely that Elon Musk could have a conversation with George Washington.\n",
            "\n",
            "George Washington lived in a time when technology was significantly less advanced compared to the present day. The concepts, ideas, and knowledge that Elon Musk possesses would be far beyond what George Washington could comprehend or engage with.\n",
            "\n",
            "Additionally, the language and cultural differences between the two individuals would pose a significant barrier to effective communication. George Washington spoke English, but the language has evolved and changed over the centuries. Musk's modern vocabulary, slang, and references would likely be confusing and unfamiliar to Washington.\n",
            "\n",
            "Furthermore, the vast differences in their life experiences, societal contexts, and worldviews would make it challenging for them to find common ground or have meaningful discussions. Their perspectives on politics, science, technology, and society would be vastly different due to the vast historical and cultural gaps between them.\n",
            "\n",
            "In conclusion, while it may be theoretically possible to imagine a scenario where Elon Musk could have a\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's change the prompt."
      ],
      "metadata": {
        "id": "mbqm52kHarID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a professional social media manager who can write great posts in linkedin to increase appeal of persons profile: {request}\n",
        "\n",
        "Story:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"request\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ],
      "metadata": {
        "id": "Uoq5C6eMT5jr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''I have passed a course in large language models (1 month duration). Write a post about that.''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo1mo_8OT5nI",
        "outputId": "7ff949b1-c00f-47f2-a9f6-1f0ec72bfb61"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "I have passed a course in large language models (1 month duration). Write a post about that.\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mI have passed a course in large language models (1 month duration)\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mI have passed a course in large language models (1 month duration)\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3müéâ Exciting News! üéâ\n",
            "\n",
            "I am thrilled to announce that I have successfully completed a comprehensive course in large language models! üìö‚ú® Over the past month, I have delved deep into the world of cutting-edge technology and honed my skills in harnessing the power of these incredible models. üåü\n",
            "\n",
            "During this course, I have gained a profound understanding of how large language models work and their immense potential in transforming various industries. From natural language processing to text generation, these models have the ability to revolutionize the way we communicate and interact with technology. üöÄüí°\n",
            "\n",
            "Through hands-on projects and engaging discussions, I have not only learned how to effectively utilize these models but also discovered innovative ways to apply them in real-world scenarios. The course has equipped me with the knowledge and expertise to leverage large language models to enhance content creation, improve customer experiences, and drive meaningful engagement. üìùüí¨\n",
            "\n",
            "I am incredibly grateful for the opportunity to learn from industry experts and collaborate with fellow enthusiasts who share the same passion for this groundbreaking technology. The course has not only expanded my technical skills but also broadened my horizons, allowing me to stay at the forefront of the latest advancements in the field. üåê\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question to the best of your abilities but if you are not sure then answer you don't know: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ],
      "metadata": {
        "id": "c2c6EByDT5rQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?''')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCuIkZST5uK",
        "outputId": "3ae8c285-7a94-44c9-a987-eee8fa2b2758"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mI am stationary\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mI am looking at the wrong thing.\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3mI don't know.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition\n",
        "\n"
      ],
      "metadata": {
        "id": "lOks9w5ndiqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"{question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ],
      "metadata": {
        "id": "JydfVsh8eAFx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''Extract names and cities from the text.\\n\\n\n",
        "Output in the format: {\"names\": list of names in text, \"cities\": list of cities in text}, surname]\\n\\n\n",
        "\n",
        "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
        "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
        "from the European Cave Rescue Association, but he wasn‚Äôt able to be rescued until yesterday,\n",
        "when doctors deemed him ‚Äútransportable.‚Äù\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2jB3n87cYsj",
        "outputId": "a2ddaec6-8ebf-47db-b568-4205beab09f1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Extract names and cities from the text.\n",
            "\n",
            "\n",
            "Output in the format: {\"names\": list of names in text, \"cities\": list of cities in text}, surname]\n",
            "\n",
            "\n",
            "Mark Dickey, 40, began suffering from severe gastric pain last week after descending more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement from the European Cave Rescue Association, but he wasn‚Äôt able to be rescued until yesterday, when doctors deemed him ‚Äútransportable.‚Äù\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mnames, surname]\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mnames[Mark Dickey, cities[Southern Turkey]\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3m{\"names\": [\"Mark Dickey\"], \"cities\": [\"Southern Turkey\"]}\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question answering based on text"
      ],
      "metadata": {
        "id": "WQjad_C-dgDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''Is Mark Dickey alive?\\n\\n\n",
        "Output in the format: Yes or No, facts that prove that.\\n\\n\n",
        "\n",
        "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
        "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
        "from the European Cave Rescue Association, but he wasn‚Äôt able to be rescued until yesterday,\n",
        "when doctors deemed him ‚Äútransportable.‚Äù\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crsqwwmrdLQl",
        "outputId": "6d0b4761-b1bf-43ed-da64-7bda01d8a705"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Is Mark Dickey alive?\n",
            "\n",
            "\n",
            "Output in the format: Yes or No, facts that prove that.\n",
            "\n",
            "\n",
            "\n",
            "Mark Dickey, 40, began suffering from severe gastric pain last week after descending \n",
            "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement \n",
            "from the European Cave Rescue Association, but he wasn‚Äôt able to be rescued until yesterday, \n",
            "when doctors deemed him ‚Äútransportable.‚Äù\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mYes\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mYes\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3mYes, Mark Dickey is alive. He was rescued from Morca Cave in Southern Turkey after suffering from severe gastric pain and deemed \"transportable\" by doctors.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    }
  ]
}